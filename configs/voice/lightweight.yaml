audio:
  auto_gain_control: true
  bit_depth: 16
  channels: 1
  chunk_size: 512
  echo_cancellation: false
  format: !!python/object/apply:src.coda.components.voice.models.AudioFormat
  - wav
  noise_reduction: false
  sample_rate: 16000
  silence_duration_ms: 1000
  vad_enabled: true
  vad_threshold: 0.6
conversation_mode: !!python/object/apply:src.coda.components.voice.models.ConversationMode
- turn_based
dynamic_allocation: false
enable_traditional_pipeline: true
external_llm:
  context_window: 8192
  fallback_enabled: true
  model: llama3.1:8b-instruct-q4_K_M
  parallel_processing: false
  provider: ollama
  reasoning_mode: basic
  temperature: 0.7
  vram_allocation: 4GB
fallback_tts_model: xtts_v2
fallback_whisper_model: large-v3
memory_integration_enabled: false
mode: !!python/object/apply:src.coda.components.voice.models.VoiceProcessingMode
- traditional
moshi:
  device: cpu
  enable_streaming: false
  external_llm_enabled: false
  inner_monologue_enabled: true
  max_conversation_length: 300
  model_path: kyutai/moshika-pytorch-bf16
  optimization: int8
  target_latency_ms: 1000
  vram_allocation: 2GB
personality_integration_enabled: false
reserved_system: 2GB
tools_integration_enabled: false
total_vram: 16GB
websocket_events_enabled: false
