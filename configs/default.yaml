# Coda Default Configuration
# This is the base configuration for Coda 2.0

# Voice processing configuration
voice:
  mode: "moshi_only"
  conversation_mode: "turn_based"
  audio:
    sample_rate: 16000
    channels: 1
    bit_depth: 16
    chunk_size: 1024
    format: "wav"
    vad_enabled: true
    vad_threshold: 0.5
    silence_duration_ms: 1000
    noise_reduction: true
    echo_cancellation: true
    auto_gain_control: true
  moshi:
    model_path: "kyutai/moshiko-pytorch-bf16"
    device: "cuda"
    optimization: "bf16"
    max_conversation_length: 300
    target_latency_ms: 200
    vram_allocation: "8GB"
    enable_streaming: true
    external_llm_enabled: true
    inner_monologue_enabled: true
  external_llm:
    provider: "ollama"
    model: "qwen3:30b-a3b"
    vram_allocation: "20GB"
    reasoning_mode: "enhanced"
    context_window: 8192
    temperature: 0.7
    parallel_processing: true
    fallback_enabled: true
  memory_integration_enabled: true
  personality_integration_enabled: true
  tools_integration_enabled: true
  websocket_events_enabled: true
  total_vram: "32GB"
  reserved_system: "4GB"
  dynamic_allocation: true
  enable_traditional_pipeline: false
  fallback_whisper_model: "large-v3"
  fallback_tts_model: "xtts_v2"

# Memory system configuration
memory:
  short_term:
    max_turns: 20
    max_tokens: 800
  long_term:
    enabled: true
    storage_path: "data/memory/long_term"
    vector_db_type: "chroma"
    embedding_model: "all-MiniLM-L6-v2"
    max_memories: 1000
    device: "cpu"
    chunk_size: 200
    chunk_overlap: 50
    min_chunk_length: 50
    time_decay_days: 30.0
    auto_persist: true
    persist_interval: 5

# Personality engine configuration
personality:
  enabled: true
  base_personality: "helpful_assistant"
  adaptation_enabled: true

# Tool system configuration
tools:
  enabled: true
  available_tools:
    - "get_time"
    - "get_date"
    - "tell_joke"
    - "get_weather"
    - "search_memory"
    - "add_memory"
    - "list_tools"
    - "show_capabilities"
  enabled_tools:
    - "get_time"
    - "get_date"
    - "tell_joke"
    - "get_weather"
    - "search_memory"
    - "add_memory"
    - "list_tools"
    - "show_capabilities"

# Language model configuration
llm:
  provider: "ollama"
  model: "qwen3:30b-a3b"
  api_key: null
  base_url: "http://localhost:11434"
  temperature: 0.7
  max_tokens: 256

# Intent routing configuration
intent:
  enabled: true
  debug_mode: false

# Feedback system configuration
feedback:
  enabled: true
  frequency: 0.3     # How often to request feedback (0.0 to 1.0)
  cooldown: 5        # Minimum turns between feedback requests
  apply_adjustments: true

# Audio settings
audio:
  input_device: null   # null = default device
  output_device: null  # null = default device
  sample_rate: 16000
  channels: 1

# WebSocket dashboard configuration
dashboard:
  enabled: true
  host: "localhost"
  port: 8081
  cors_origins:
    - "http://localhost:3000"
    - "http://localhost:8080"
    - "http://localhost:8081"

# Logging configuration
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  file: "logs/coda.log"
  max_size: 10485760  # 10 MB
  backup_count: 5
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Performance settings
performance:
  max_concurrent_users: 1  # Will increase with Kyutai integration
  response_timeout: 30     # Maximum response time in seconds
  memory_cleanup_interval: 300  # Memory cleanup interval in seconds

# Development settings (only used in dev mode)
development:
  debug_mode: false
  mock_services: false
  test_data_path: "tests/data"
  profiling_enabled: false

# Production settings
production:
  monitoring_enabled: false
  metrics_endpoint: "/metrics"
  health_check_endpoint: "/health"
  graceful_shutdown_timeout: 30
