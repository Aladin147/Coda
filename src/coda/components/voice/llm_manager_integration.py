"""
Voice-LLM Manager Integration

This module provides comprehensive integration between the voice processing system
and the existing LLM manager for seamless conversation handling and response generation.
"""

import asyncio
import logging
import time
from typing import Dict, List, Any, Optional, Tuple, AsyncGenerator, Union
from dataclasses import dataclass
from datetime import datetime

from .models import VoiceMessage, VoiceResponse, ConversationState, VoiceProcessingMode
from ..llm.interfaces import LLMManagerInterface
from ..llm.models import (
    LLMMessage, LLMResponse, LLMStreamChunk, MessageRole,
    LLMProvider, LLMConfig, ProviderConfig
)
from ..llm.manager import LLMManager

logger = logging.getLogger("coda.voice.llm_manager_integration")


@dataclass
class VoiceLLMManagerConfig:
    """Configuration for voice-LLM manager integration."""
    
    # LLM Manager Settings
    use_existing_llm_manager: bool = True
    llm_provider: str = "ollama"
    llm_model: str = "gemma3:1b"
    
    # Voice-specific LLM settings
    voice_conversation_prefix: str = "voice_"
    enable_voice_context_injection: bool = True
    voice_response_max_tokens: int = 300  # Shorter for voice
    voice_temperature: float = 0.8  # More conversational
    
    # Integration settings
    enable_streaming_for_voice: bool = True
    voice_timeout_seconds: float = 8.0
    enable_conversation_continuity: bool = True
    
    # Voice-specific prompt engineering
    enable_voice_prompt_adaptation: bool = True
    voice_system_prompt: str = (
        "You are a helpful AI assistant in a voice conversation. "
        "Keep responses concise, natural, and conversational. "
        "Avoid overly long explanations unless specifically requested."
    )
    
    # Performance optimization
    enable_response_caching: bool = True
    cache_ttl_minutes: int = 10
    parallel_llm_requests: bool = False  # Voice is sequential


class VoiceLLMManagerIntegration:
    """
    Comprehensive integration between voice processing and LLM manager.
    
    Features:
    - Seamless integration with existing LLM manager
    - Voice-optimized conversation handling
    - Streaming response support for voice
    - Voice-specific prompt engineering
    - Conversation continuity across voice sessions
    - Performance optimization for real-time voice
    """
    
    def __init__(
        self,
        llm_manager: LLMManagerInterface,
        config: VoiceLLMManagerConfig
    ):
        """Initialize voice-LLM manager integration."""
        self.llm_manager = llm_manager
        self.config = config
        
        # Voice conversation tracking
        self.voice_conversations: Dict[str, str] = {}  # voice_conv_id -> llm_conv_id
        self.conversation_contexts: Dict[str, Dict[str, Any]] = {}
        
        # Response caching
        self.response_cache: Dict[str, VoiceResponse] = {}
        self.cache_timestamps: Dict[str, datetime] = {}
        
        # Statistics tracking
        self.stats = {
            "voice_llm_requests": 0,
            "streaming_responses": 0,
            "cache_hits": 0,
            "conversation_continuations": 0,
            "average_response_time_ms": 0.0,
            "total_tokens_used": 0
        }
        
        logger.info("VoiceLLMManagerIntegration initialized")
    
    async def generate_voice_response(
        self,
        voice_message: VoiceMessage,
        conversation_state: Optional[ConversationState] = None,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> VoiceResponse:
        """
        Generate voice response using the LLM manager.
        
        Args:
            voice_message: The voice message to respond to
            conversation_state: Current conversation state
            enhanced_context: Enhanced context from memory, personality, tools
            
        Returns:
            Voice response generated by LLM
        """
        
        start_time = time.time()
        
        try:
            # Check cache first
            cache_key = self._generate_cache_key(voice_message, enhanced_context)
            if self._is_response_cached(cache_key):
                self.stats["cache_hits"] += 1
                return self._get_cached_response(cache_key)
            
            # Get or create LLM conversation
            llm_conversation_id = await self._get_llm_conversation_id(voice_message.conversation_id)
            
            # Prepare voice-optimized prompt
            prompt = await self._prepare_voice_prompt(voice_message, enhanced_context)
            
            # Generate LLM response
            llm_response = await self.llm_manager.continue_conversation(
                conversation_id=llm_conversation_id,
                message=prompt,
                stream=False,  # Non-streaming for now
                provider=self.config.llm_provider,
                model=self.config.llm_model,
                max_tokens=self.config.voice_response_max_tokens,
                temperature=self.config.voice_temperature,
                timeout=self.config.voice_timeout_seconds
            )
            
            # Convert LLM response to voice response
            voice_response = await self._convert_llm_to_voice_response(
                llm_response, voice_message, start_time
            )
            
            # Cache the response
            self._cache_response(cache_key, voice_response)
            
            # Update statistics
            self.stats["voice_llm_requests"] += 1
            processing_time = (time.time() - start_time) * 1000
            self._update_average_response_time(processing_time)
            
            if hasattr(llm_response, 'usage') and llm_response.usage:
                self.stats["total_tokens_used"] += llm_response.usage.total_tokens
            
            logger.debug(f"Generated voice response in {processing_time:.1f}ms")
            
            return voice_response
            
        except Exception as e:
            logger.error(f"Failed to generate voice response: {e}")
            
            # Create fallback response
            return await self._create_fallback_response(voice_message, str(e))
    
    async def generate_streaming_voice_response(
        self,
        voice_message: VoiceMessage,
        conversation_state: Optional[ConversationState] = None,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[VoiceResponse, None]:
        """
        Generate streaming voice response using the LLM manager.
        
        Args:
            voice_message: The voice message to respond to
            conversation_state: Current conversation state
            enhanced_context: Enhanced context from memory, personality, tools
            
        Yields:
            Streaming voice response chunks
        """
        
        start_time = time.time()
        
        try:
            if not self.config.enable_streaming_for_voice:
                # Fall back to non-streaming
                response = await self.generate_voice_response(
                    voice_message, conversation_state, enhanced_context
                )
                yield response
                return
            
            # Get or create LLM conversation
            llm_conversation_id = await self._get_llm_conversation_id(voice_message.conversation_id)
            
            # Prepare voice-optimized prompt
            prompt = await self._prepare_voice_prompt(voice_message, enhanced_context)
            
            # Generate streaming LLM response
            accumulated_content = ""
            chunk_count = 0
            
            async for llm_chunk in self.llm_manager.continue_conversation(
                conversation_id=llm_conversation_id,
                message=prompt,
                stream=True,
                provider=self.config.llm_provider,
                model=self.config.llm_model,
                max_tokens=self.config.voice_response_max_tokens,
                temperature=self.config.voice_temperature,
                timeout=self.config.voice_timeout_seconds
            ):
                chunk_count += 1
                
                if hasattr(llm_chunk, 'delta') and llm_chunk.delta:
                    accumulated_content += llm_chunk.delta
                    
                    # Create voice response chunk
                    voice_chunk = VoiceResponse(
                        response_id=f"{voice_message.message_id}_chunk_{chunk_count}",
                        conversation_id=voice_message.conversation_id,
                        message_id=voice_message.message_id,
                        text_content=accumulated_content,
                        audio_data=b"",  # Audio generation would happen separately
                        processing_mode=VoiceProcessingMode.HYBRID,
                        total_latency_ms=(time.time() - start_time) * 1000,
                        llm_latency_ms=(time.time() - start_time) * 1000,
                        response_relevance=0.8  # Default relevance
                    )
                    
                    yield voice_chunk
            
            # Update statistics
            self.stats["streaming_responses"] += 1
            self.stats["voice_llm_requests"] += 1
            
            logger.debug(f"Generated {chunk_count} streaming chunks")
            
        except Exception as e:
            logger.error(f"Failed to generate streaming voice response: {e}")
            
            # Yield fallback response
            fallback_response = await self._create_fallback_response(voice_message, str(e))
            yield fallback_response
    
    async def _get_llm_conversation_id(self, voice_conversation_id: str) -> str:
        """Get or create LLM conversation ID for voice conversation."""
        
        if voice_conversation_id in self.voice_conversations:
            return self.voice_conversations[voice_conversation_id]
        
        # Create new LLM conversation
        llm_conversation_id = f"{self.config.voice_conversation_prefix}{voice_conversation_id}"
        
        # Initialize conversation with voice system prompt
        if self.config.enable_voice_prompt_adaptation:
            await self.llm_manager.generate_response(
                prompt="Initialize voice conversation",
                conversation_id=llm_conversation_id,
                system_prompt=self.config.voice_system_prompt,
                provider=self.config.llm_provider
            )
        
        self.voice_conversations[voice_conversation_id] = llm_conversation_id
        self.conversation_contexts[voice_conversation_id] = {
            "created_at": datetime.now(),
            "message_count": 0,
            "last_activity": datetime.now()
        }
        
        logger.debug(f"Created LLM conversation {llm_conversation_id} for voice conversation {voice_conversation_id}")
        
        return llm_conversation_id
    
    async def _prepare_voice_prompt(
        self,
        voice_message: VoiceMessage,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Prepare voice-optimized prompt for LLM."""
        
        base_prompt = voice_message.text_content or ""
        
        if not enhanced_context:
            return base_prompt
        
        # Build enhanced prompt with context
        prompt_parts = []
        
        # Add memory context
        if enhanced_context.get("memory") and self.config.enable_voice_context_injection:
            memory_context = enhanced_context["memory"]
            relevant_memories = memory_context.get("relevant_memories", [])
            
            if relevant_memories:
                memory_summary = memory_context.get("context_summary", "")
                prompt_parts.append(f"[Context: {memory_summary}]")
        
        # Add personality context
        if enhanced_context.get("personality"):
            personality_context = enhanced_context["personality"]
            personality_desc = personality_context.get("description", "")
            
            if personality_desc:
                prompt_parts.append(f"[Personality: {personality_desc}]")
        
        # Add tools context
        if enhanced_context.get("tools"):
            tools_context = enhanced_context["tools"]
            tools_summary = tools_context.get("tools_summary", "")
            
            if tools_summary:
                prompt_parts.append(f"[Available tools: {tools_summary}]")
        
        # Combine prompt parts
        if prompt_parts:
            enhanced_prompt = "\n".join(prompt_parts) + "\n\nUser: " + base_prompt
        else:
            enhanced_prompt = base_prompt
        
        return enhanced_prompt
    
    async def _convert_llm_to_voice_response(
        self,
        llm_response: LLMResponse,
        voice_message: VoiceMessage,
        start_time: float
    ) -> VoiceResponse:
        """Convert LLM response to voice response."""
        
        processing_time = (time.time() - start_time) * 1000
        
        voice_response = VoiceResponse(
            response_id=f"llm_{llm_response.id}",
            conversation_id=voice_message.conversation_id,
            message_id=voice_message.message_id,
            text_content=llm_response.content,
            audio_data=b"",  # Audio generation would happen separately
            processing_mode=VoiceProcessingMode.HYBRID,
            total_latency_ms=processing_time,
            llm_latency_ms=processing_time,
            response_relevance=0.8  # Could be calculated based on LLM confidence
        )
        
        return voice_response
    
    async def _create_fallback_response(
        self,
        voice_message: VoiceMessage,
        error_message: str
    ) -> VoiceResponse:
        """Create fallback response when LLM fails."""
        
        fallback_text = (
            "I'm having trouble processing your request right now. "
            "Could you please try again?"
        )
        
        return VoiceResponse(
            response_id=f"fallback_{voice_message.message_id}",
            conversation_id=voice_message.conversation_id,
            message_id=voice_message.message_id,
            text_content=fallback_text,
            audio_data=b"",
            processing_mode=VoiceProcessingMode.HYBRID,
            total_latency_ms=10.0,
            llm_latency_ms=10.0,
            response_relevance=0.3
        )
    
    def _generate_cache_key(
        self,
        voice_message: VoiceMessage,
        enhanced_context: Optional[Dict[str, Any]]
    ) -> str:
        """Generate cache key for response caching."""
        
        text_hash = hash(voice_message.text_content or "")
        context_hash = hash(str(enhanced_context)) if enhanced_context else 0
        
        return f"voice_llm_{voice_message.conversation_id}_{text_hash}_{context_hash}"
    
    def _is_response_cached(self, cache_key: str) -> bool:
        """Check if response is cached and valid."""
        
        if not self.config.enable_response_caching:
            return False
        
        if cache_key not in self.response_cache:
            return False
        
        cache_time = self.cache_timestamps.get(cache_key)
        if not cache_time:
            return False
        
        cache_age = (datetime.now() - cache_time).total_seconds() / 60
        return cache_age < self.config.cache_ttl_minutes
    
    def _get_cached_response(self, cache_key: str) -> VoiceResponse:
        """Get cached response."""
        return self.response_cache[cache_key]
    
    def _cache_response(self, cache_key: str, response: VoiceResponse) -> None:
        """Cache response."""
        if self.config.enable_response_caching:
            self.response_cache[cache_key] = response
            self.cache_timestamps[cache_key] = datetime.now()
            
            # Clean old cache entries
            self._cleanup_cache()
    
    def _cleanup_cache(self) -> None:
        """Clean up expired cache entries."""
        current_time = datetime.now()
        expired_keys = []
        
        for cache_key, timestamp in self.cache_timestamps.items():
            cache_age = (current_time - timestamp).total_seconds() / 60
            if cache_age > self.config.cache_ttl_minutes:
                expired_keys.append(cache_key)
        
        for key in expired_keys:
            self.response_cache.pop(key, None)
            self.cache_timestamps.pop(key, None)
    
    def _update_average_response_time(self, new_time_ms: float) -> None:
        """Update average response time."""
        current_avg = self.stats["average_response_time_ms"]
        request_count = self.stats["voice_llm_requests"]
        
        if request_count == 1:
            self.stats["average_response_time_ms"] = new_time_ms
        else:
            # Running average
            self.stats["average_response_time_ms"] = (
                (current_avg * (request_count - 1) + new_time_ms) / request_count
            )
    
    async def get_conversation_history(
        self,
        voice_conversation_id: str,
        limit: int = 10
    ) -> List[LLMMessage]:
        """Get conversation history from LLM manager."""
        
        if voice_conversation_id not in self.voice_conversations:
            return []
        
        llm_conversation_id = self.voice_conversations[voice_conversation_id]
        
        try:
            conversation = await self.llm_manager.get_conversation(llm_conversation_id)
            if conversation and conversation.messages:
                return conversation.messages[-limit:]
            return []
        except Exception as e:
            logger.error(f"Failed to get conversation history: {e}")
            return []
    
    def get_integration_stats(self) -> Dict[str, Any]:
        """Get voice-LLM integration statistics."""
        
        return {
            "voice_llm_stats": self.stats.copy(),
            "active_conversations": len(self.voice_conversations),
            "cache_size": len(self.response_cache),
            "config": {
                "llm_provider": self.config.llm_provider,
                "llm_model": self.config.llm_model,
                "streaming_enabled": self.config.enable_streaming_for_voice,
                "caching_enabled": self.config.enable_response_caching,
                "voice_timeout": self.config.voice_timeout_seconds,
                "max_tokens": self.config.voice_response_max_tokens
            }
        }
    
    async def cleanup(self) -> None:
        """Clean up integration resources."""
        
        try:
            # Clear caches
            self.response_cache.clear()
            self.cache_timestamps.clear()
            
            # Clear conversation mappings
            self.voice_conversations.clear()
            self.conversation_contexts.clear()
            
            logger.info("VoiceLLMManagerIntegration cleanup completed")
            
        except Exception as e:
            logger.error(f"Cleanup failed: {e}")


class VoiceLLMManager:
    """
    High-level manager for voice-LLM integration.
    
    Provides a simplified interface for voice components to interact with LLM manager.
    """
    
    def __init__(
        self,
        llm_manager: LLMManagerInterface,
        config: Optional[VoiceLLMManagerConfig] = None
    ):
        """Initialize voice LLM manager."""
        self.llm_manager = llm_manager
        self.config = config or VoiceLLMManagerConfig()
        self.integration = VoiceLLMManagerIntegration(llm_manager, self.config)
        
        logger.info("VoiceLLMManager initialized")
    
    async def generate_voice_response(
        self,
        voice_message: VoiceMessage,
        conversation_state: Optional[ConversationState] = None,
        enhanced_context: Optional[Dict[str, Any]] = None,
        stream: bool = False
    ) -> Union[VoiceResponse, AsyncGenerator[VoiceResponse, None]]:
        """
        Generate voice response using LLM manager.
        
        Args:
            voice_message: Voice message to respond to
            conversation_state: Current conversation state
            enhanced_context: Enhanced context from other systems
            stream: Whether to stream the response
            
        Returns:
            Voice response or streaming voice response
        """
        
        if stream:
            return self.integration.generate_streaming_voice_response(
                voice_message, conversation_state, enhanced_context
            )
        else:
            return await self.integration.generate_voice_response(
                voice_message, conversation_state, enhanced_context
            )
    
    async def get_conversation_history(
        self,
        voice_conversation_id: str,
        limit: int = 10
    ) -> List[LLMMessage]:
        """Get conversation history."""
        return await self.integration.get_conversation_history(voice_conversation_id, limit)
    
    def get_llm_stats(self) -> Dict[str, Any]:
        """Get comprehensive LLM statistics."""
        return self.integration.get_integration_stats()
    
    async def cleanup(self) -> None:
        """Clean up manager resources."""
        await self.integration.cleanup()
